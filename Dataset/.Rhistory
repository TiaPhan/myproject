q()
installed.packages()
install.packages(tidyverse)
install.packages("tidyverse")
data(penguins)
install.packages("Warning Messages")
data(AdultUCI)
library(arules)
data("AdultUCI")
uci=AdultUCI
View(uci)
class(age)
class('age')
supresswarnings(as.numeric(age))
supressWarnings(as.numeric(age))
warning(as.numeric(age))
warning(as.numeric('age'))
age
"age"
as.numeric(age)
as.numeric('age')
uci[["age"]]=ordered(cut(uci[[ "age"]], c(15,25,45,65,100)),labels = c("Young", "Middle-aged", "Senior", "Old"))
uci[["hours_per_week"]]=ordered(cut(uci[["hours_per_week"]],c(0,25,40,60,168),labels = c("Part-time","Full-time","Over-time","Workaholic"))
uci[["capital_gain"]]=ordered(cut(uci[["capital_gain"]][uci[["captal_gain"]]>0),Inf)),labels=c("None","Low","High"))
uci[["age"]]=ordered(cut(uci[[ "age"]], c(15,25,45,65,100)),labels = c("Young", "Middle-aged", "Senior", "Old"))
uci[["hours_per_week"]]=ordered(cut(uci[["hours_per_week"]],c(0,25,40,60,168),labels = c("Part-time","Full-time","Over-time","Workaholic"))
uci[["capital_gain"]]=ordered(cut(uci[["capital_gain"]][uci[["captal_gain"]]>0),Inf)),labels=c("None","Low","High"))
summary(uci)
uci[["hours_per_week"]]=ordered(cut(uci[["hours_per_week"]],c(0,25,40,60,168),labels = c("Part-time","Full-time","Over-time","Workaholic"))
uci[["capital_gain"]]=ordered(cut(uci[["capital_gain"]][uci[["captal_gain"]]>0),Inf)),labels=c("None","Low","High"))
uci[["hours_per_week"]]=ordered(cut(uci[["hours_per_week"]],c(0,25,40,60,168)),labels = c("Part-time","Full-time","Over-time","Workaholic"))
uci[["capital_gain"]]=ordered(cut(uci[["capital_gain"]][uci[["captal_gain"]]>0),Inf)),labels=c("None","Low","High"))
uci[["capital-gain"]]=ordered(cut(uci[["capital-gain"]][uci[["captal-gain"]]>0),Inf)),labels=c("None","Low","High"))
uci[["capital-gain"]]=ordered(cut(uci[["capital-gain"]],c(-Inf,0,median(uci[["capital-gain"]][uci[["captal-gain"]]>0),Inf)),labels=c("None","Low","High"))
summary(uci)
uci[["hours_per_week"]]=ordered(cut(uci[["hours_per_week"]],c(0,25,40,60,168)),labels = c("Part-time","Full-time","Over-time","Workaholic"))
as.numeric('hours_per_week')
uci[["hours_per_week"]]=ordered(cut(uci[["hours_per_week"]],c(0,25,40,60,168)),labels = c("Part-time","Full-time","Over-time","Workaholic"))
library(Warning Messages)
library("Warning Messages")
class(hours_per_week)
class('hours_per_week')
uci[["hours_per_week"]]=ordered(cut(uci[[ "hours_per_week"]], c(0,25,40,60,168)),labels = c("Part-time","Full-time","Over-time","Workaholic"))
as.numeric('hours_per_week')
warning(as.numeric('hours_per_week'))
supresswarning(as.numeric('hours_per_week'))
as.factor(uci$`hours-per-week`)
uci[["hours_per_week"]]=ordered(cut(uci[[ "hours_per_week"]], c(0,25,40,60,168)),labels = c("Part-time","Full-time","Over-time","Workaholic"))
as.numeric(uci$`hours-per-week`)
uci[["hours_per_week"]]=ordered(cut(uci[[ "hours_per_week"]], c(0,25,40,60,168)),labels = c("Part-time","Full-time","Over-time","Workaholic"))
age
uci$age
class(age)
class('age')
uci[["hours-per-week"]]=ordered(cut(uci[["hours-per-week"]],c(0,25,40,168)),labels=c("Part-time","Full-time","Over-time","Workaholic"))
uci[["hours-per-week"]]=ordered(cut(uci[["hours-per-week"]],c(0,25,40,60,168)),labels=c("Part-time","Full-time","Over-time","Workaholic"))
uci[["capital-gain"]]=ordered(cut(uci[["capital-gain"]],c(-Inf,0,median(uci[["capital-gain"]][uci[["captal-gain"]]>0),Inf)),labels=c("None","Low","High"))
uci[["capital-gain"]]=ordered(cut(uci[["capital-gain"]],c(-Inf,0,median(uci[["capital-gain"]],Inf)),labels=c("None","Low","High"))
uci[["capital-gain"]]=ordered(cut(uci[["capital-gain"]],c(-Inf,0,median(uci[["capital-gain"]],Inf)),labels=c("None","Low","High"))
uci[["capital-gain"]]=ordered(cut(uci[["capital-gain"]],c(-Inf,0,median(uci[["capital-gain"]][uci[["capital-gain"]]>0],Inf)),labels=c("None","Low","High"))
uci[["capital-gain"]]=ordered(cut(uci[["capital-gain"]],c(-Inf,0,median(uci[["capital-gain"]][uci[["capital-gain"]]>0],Inf)),labels=c("None","Low","High"))
uci[["capital-gain"]]=ordered(cut(uci[["capital-gain"]],c(-Inf,0,median(uci[["capital-gain"]][uci[["capital-gain"]]>0],Inf)),labels=c("None","Low","High"))
uci[["capital-gain"]]=ordered(cut(uci[["capital-gain"]],c(-Inf,0,median(uci[["capital-gain"]][uci[["capital-gain"]]>0],Inf)),labels=c("None","Low","High"))
uci[["capital-gain"]]=ordered(cut(uci[["capital-gain"]],c(-Inf,0,median(uci[["capital-gain"]],Inf),labels=c("None","Low","High"))
summary(uci)
summary(uci)
uci[["capital-gain"]]=ordered(cut(uci[["capital-gain"]],c(-Inf,0,median(uci[["capital-gain"]][uci[[capital-gain]]>0],Inf),labels=c("None","Low","High"))
uci[["capital-gain"]]=ordered(cut(uci[["capital-gain"]],c(-Inf,0,median(uci[["capital-gain"]][uci[[capital-gain]]>0],Inf),labels=c("None","Low","High"))
AdultUCI[[ "capital-gain"]] <- ordered(cut(AdultUCI[[ "capital-gain"]],
+ c(-Inf,0,median(AdultUCI[[ "capital-gain"]][AdultUCI[[ "capital-gain"]]>0]),Inf)),labels = c("None", "Low", "High"))
uci[[ "capital-gain"]] <- ordered(cut(uci[[ "capital-gain"]],c(-Inf,0,median(uci[[ "capital-gain"]][uci[[ "capital-gain"]]>0]),Inf)),labels = c("None", "Low", "High"))
uci(1:2,)
data("AdultUCI")
library('arules')
data("AdultUCI")
uci=AdultUCI
uci(1:2,)
uci[1:2,]
uci(1:2)
uci[1:2]
uci[["fnlwgt"]] = NULL
uci[["education-num"]] = NULL
uci[["age"]] = ordered(cut(uci[["age"]], c(15,25,45,65,100)), labels=c("Young","Middle-age","Senior","Old"))
uci[["hours-per-week"]] = ordered(cut(uci[["hours-per-week"]], c(0,25,40,60,168)), labels=c("Part-time", "Full-time","Overtime","Workaholic"))
uci[["capital-gain"]] = ordered(cut(uci[["capital-gain"]], c(-Inf,0, median(uci[["capital-gain"]][uci[["capital-gain"]]>0]), Inf)), labels=c("None", "Low", "High"))
uci[["capital-loss"]] = ordered(cut(uci[["capital-loss"]],c(-Inf,0,median(uci[["capital-loss"]][uci[["capital-loss"]]>0]),Inf)), labels=c("None", "Low", "High"))
str(uci)
adult=(uci,"transaction")
adult=as(uci,"transaction")
adult=as(uci,"transactions")
summary(adult)
itemFrequencyPlot(adult)
window()
itemFrequencyPlot(adult)
window()
windows()
itemFrequencyPlot(adult)
windows()
itemFrequencyPlot(adult)
rules=apriori(adult,parameter = list(supp=0.01, conf=0.06 ))
rules_income_small=subset(rules,subset = rhs%in% "income=small"&lift>1.2)
rules_income_large=subset(rules,subset = rhs%in% "income_large"&lift>1.2)
rules_income_large=subset(rules,subset = rhs%in% "income=large"&lift>1.2)
inspect(head(rules_income_small, n=3, by = "confidence")
show(rules_income_small)
inspect(head(rules_income_small, n=3, by = "confidence"))
rules_income_small
inspect(head(rules_income_large, n=3, by = "confidence"))
inspect(head(rules_income_small, n=3, by = "confidence"))
knitr::opts_chunk$set(echo = TRUE)
inspect(head(rules_income_small, n=3, by = "confidence"))
data("retail")
plot("retail")
library(ggplot2)
plot("retail")
View("retail")
data("Retail Market Basket ")
data("Retail Market")
install.packages(packrat)
install.packages("packrat")
library(C50)
computer=read.csv("da_computer3.txt")
attach(computer)
?attach  #attach your database to R
?na.omit #removed all NAs
ff=na.omit(computer)
ftree=tree(age~., data=ff);ftree
names(ff)
prop.table(table(ff$buy)) #buying percentage in ff dataset
install.packages("tree")
library(tree)
?trees
library(dplyr)
library(tidyr)
ff=ff%>%separate(edu.age.marri.income.city.buy, c("edu","age","marri","income","city","buy"))
names(ff)
ftree=tree(buy~.,data=ff);ftree
summary(ftree)
class(ftree)
head(ftree)
plot(ftree)
text(ftree)
show(ftree)
ind <- sample(2,nrow(ff),replace=TRUE,prob=c(0.7,0.3)) #The common practice is to split the data 80/20, 80 percent of the data serves to train the model, and 20 percent to make predictions.
?sample
traindata <- ff[ind==1,]
traindata
testdata = ff[ind==2,]
testdata
create_buy_test <- function(ff, size = 0.8, buy = TRUE) {
n_row = nrow(ff)
total_row = size * n_row
buy_sample < - 1: total_row
if (train == TRUE) {
return (data[ff, ])
} else {
return (data[-ff, ])
}
}
data_test <- create_buy_test(buy, 0.8, buy = FALSE)
prop.table(table(traindata$buy)) #buying percentage in traindata set
install.packages("rpart")    #regression plot
install.packages("rpart.plot")
library(rpart)
library(rpart.plot)
fit <- rpart(buy~., data = traindata, method = 'class')
rpart.plot(fit, extra = 106)
predict_unseen <-predict(fit, testdata, type = 'class')  #predict which passengers are more likely to survive after the collision from the test set. It means, you will know among those 209 passengers, which one will buy or not.
table_mat <- table(testdata$buy, predict_unseen)
install.packages("rpart")
COVID19_line_list_data <- read.csv("C:/Users/Tia Phan/Downloads/COVID19_line_list_data.csv")
View(COVID19_line_list_data)
data<- read.csv("C:/Users/Tia Phan/Downloads/COVID19_line_list_data.csv")
describe(data)
install.packages(Hmisc)
install.packages("Hmisc")
install.packages("Hmisc")
data<- read.csv("C:/Users/Tia Phan/Downloads/COVID19_line_list_data.csv")
describe(data)
describe
library(Hmisc)
describe(data)
data$death_dummy=as.integer(data$death != 0)
dead=subset(data,death_dummy==1)
alive=subset(data,death_dummy==0)
mean(dead$age)
mean(dead$age, na.rm = TRUE)
mean(alive$age, na.rm=TRUE)
t.test(dead$sge,alive$age,alternative="two.sided",conf.level=0.99)
t.test(dead$age,alive$age,alternative="two.sided",conf.level=0.99)
t.test(dead$age,alive$age,alternative="two.sided",conf.level=0.5)
male=subset(data,gender=="male")
female=subset(data,gender=="female")
mean(female$death_dummy)
mean(male$death_dummy)
t.test(male$death_dummy,female$death_dummy,alternative = "two.sided",conf.level = 0.95)
survey=read_excel('Survey.xlxs')
survey=read.xlsx('Survey.xlsx')
install.packages("tseries")
install.packages("timeSeries")
install.packages("forecast")
tw0050 <- getSymbols("0050.TW", auto.assign = FALSE, from="2021-05" ,to="2022-05-01")
tw0050 <- getSymbols("0050.TW", auto.assign = FALSE, from="2020-05" ,to="2022-05")
tw0050 <- getSymbols("0050.TW", auto.assign = FALSE, from="2020-05" to="2022-05")
tw0050 <- getSymbols("0050.TW", auto.assign = FALSE, from="2000-01-01" ,to="2022-05-01")
tw0050 <- getSymbols("0050.TW", auto.assign = FALSE, from="2000-01-01" ,to="2022-05-01")
install.packages("tseries")
install.packages("timeSeries")
install.packages("forecast")
sp500=new.env()
gspc <- getSymbols("GSPC", env = sp500, src = "yahoo", from = as.Date("1960-01-04"), to = as.Date("2009-01-01"))
library(quantmod)
sp500=new.env()
gspc <- getSymbols("GSPC", env = sp500, src = "yahoo", from = as.Date("1960-01-04"), to = as.Date("2009-01-01"))
install.packages('mlr3')
library(mlr3)
install.packages("MASS")
install.packages("caTools")
install.packages("neuralnet")
install.packages("knitr")
library(MASS)
library(caTools)
library(neuralnet)
library(ggplot2)
library(knitr)
head(Boston)
is.na(Boston)
sum(is.na(Boston))
split <- sample.split(scaled$medv, SplitRatio = 0.75)
train <- subset(scaled, split == T)
test <- subset(scaled, split == F)
scaled.data <- scale(data, center = mins, scale = maxs - mins)
scaled <- as.data.frame(scaled.data)
#Scale data
scaled.data <- scale(data, center = mins, scale = maxs - mins)
mins <- apply(data, MARGIN=2, min)
maxs <- apply(data, MARGIN=2, max)
any(is.na(Boston))
data <- Boston
maxs <- apply(data, MARGIN=2, max)
mins <- apply(data, MARGIN=2, min)
scaled.data <- scale(data, center = mins, scale = maxs - mins)
scaled <- as.data.frame(scaled.data)
split <- sample.split(scaled$medv, SplitRatio = 0.75)
train <- subset(scaled, split == T)
test <- subset(scaled, split == F)
n <- names(train)
f <- as.formula(paste("medv ~", paste(n[!n %in% "medv"], collapse = " + ")))
nn <- neuralnet(f, data = train, hidden = c(5,3), linear.output = TRUE)
plot(nn)
predicted.nn.values <- compute(nn, test[1:13])
str(predicted.nn.values)
true.predictions <- predicted.nn.values$net.result *
(max(data$medv) - min(data$medv)) + min(data$medv)
set.seed(12345)
fit.lm <- lm(medv~.,data = train)
rmse.lm <- sqrt(sum((pred.lm - testing$medv)^2)/
length(testing$MEDV))
c(RMSE = rmse.lm, R2 = summary(fit.lm)$r.squared)
pred.lm <- predict(fit.lm, newdata = test)
rmse.lm <- sqrt(sum((pred.lm - testing$medv)^2)/
length(testing$MEDV))
c(RMSE = rmse.lm, R2 = summary(fit.lm)$r.squared)
rmse.lm <- sqrt(sum((pred.lm - test$medv)^2)/
length(test$medv))
c(RMSE = rmse.lm, R2 = summary(fit.lm)$r.squared)
test.r <- (test$medv)*(max(data$medv)-min(data$medv))+min(data$medv)
MSE.nn <- sum((test.r - true.predictions)^2)/nrow(test_)
test.r <- (test$medv)*(max(data$medv)-min(data$medv))+min(data$medv)
MSE.nn <- sum((test.r - true.predictions)^2)/nrow(test)
print(paste(MSE.lm,MSE.nn))
library(MASS)
library(caTools)
library(neuralnet)
library(ggplot2)
library(knitr)
any(is.na(Boston))
set.seed(500)
data=Boston
apply(data,2,function(x) sum(is.na(x)))
index <- sample(1:nrow(data),round(0.75*nrow(data)))
train <- data[index,]
test <- data[-index,]
lm.fit <- glm(medv~., data=train)
summary(lm.fit)
pr.lm <- predict(lm.fit,test)
MSE.lm <- sum((pr.lm - test$medv)^2)/nrow(test)
maxs <- apply(data, 2, max)
mins <- apply(data, 2, min)
scaled <- as.data.frame(scale(data, center = mins, scale = maxs - mins))
train_ <- scaled[index,]
test_ <- scaled[-index,]
n <- names(train_)
f <- as.formula(paste("medv ~", paste(n[!n %in% "medv"], collapse = " + ")))
nn <- neuralnet(f,data=train_,hidden=c(5,3),linear.output=T)
plot(nn)
pr.nn <- compute(nn,test_[,1:13])
pr.nn_ <- pr.nn$net.result*(max(data$medv)-min(data$medv))+min(data$medv)
test.r <- (test_$medv)*(max(data$medv)-min(data$medv))+min(data$medv)
MSE.nn <- sum((test.r - pr.nn_)^2)/nrow(test_)
print(paste(MSE.lm,MSE.nn))
par(mfrow=c(1,2))
plot(test$medv,pr.nn_,col='red',main='Real vs predicted NN',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='NN',pch=18,col='red', bty='n')
plot(test$medv,pr.lm,col='blue',main='Real vs predicted lm',pch=18, cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='LM',pch=18,col='blue', bty='n', cex=.95)
plot(test$medv,pr.lm,col='blue',main='Real vs predicted lm',pch=18, cex=0.7)
abline(0,1,lwd=2)
plot(test$medv,pr.lm,col='blue',main='Real vs predicted lm',pch=18, cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='LM',pch=18,col='blue', bty='n', cex=.95)
plot(test$medv,pr.nn_,col='red',main='Real vs predicted NN',pch=18,cex=0.7)
points(test$medv,pr.lm,col='blue',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend=c('NN','LM'),pch=18,col=c('red','blue'))
library(boot)
set.seed(200)
lm.fit <- glm(medv~.,data=data)
cv.glm(data,lm.fit,K=10)$delta[1]
set.seed(450)
cv.error <- NULL
k <- 10
library(plyr)
pbar <- create_progress_bar('text')
pbar$init(k)
for(i in 1:k){
index <- sample(1:nrow(data),round(0.9*nrow(data)))
train.cv <- scaled[index,]
test.cv <- scaled[-index,]
nn <- neuralnet(f,data=train.cv,hidden=c(5,2),linear.output=T)
pr.nn <- compute(nn,test.cv[,1:13])
pr.nn <- pr.nn$net.result*(max(data$medv)-min(data$medv))+min(data$medv)
test.cv.r <- (test.cv$medv)*(max(data$medv)-min(data$medv))+min(data$medv)
cv.error[i] <- sum((test.cv.r - pr.nn)^2)/nrow(test.cv)
pbar$step()
}
mean(cv.error)
cv.error
boxplot(cv.error,xlab='MSE CV',col='cyan',
border='blue',names='CV error (MSE)',
main='CV error (MSE) for NN',horizontal=TRUE)
plot(nn)
library(readr)
hp1 <- read_csv("C:/Users/Tia Phan/OneDrive/Máy tính/a/Dataset/Happiness Score/new-world-happiness-report-2021.csv")
View(hp1)
library(ggplot2)
library(dplyr)
library(arules)
cor(hp1[, unlist(lapply(hp1, is.numeric))])
# Correlation
plot(hp1, main="Correlation")
hp1$`Ladder score in Dystopia`=NULL
hp1$`Explained by: Log GDP per capita`=NULL
hp1$`Explained by: Social support`=NULL
hp1$`Explained by: Healthy life expectancy`=NULL
hp1$`Explained by: Freedom to make life choices`=NULL
hp1$`Explained by: Generosity`=NULL
hp1$`Explained by: Perceptions of corruption`=NULL
hp1$`Standard error of ladder score`=NULL
hp1$upperwhisker=NULL
hp1$lowerwhisker=NULL
hp1$`Dystopia + residual`=NULL
# Correlation
plot(hp1, main="Correlation")
heatmap(hp1)
cor(hp1[, unlist(lapply(hp1, is.numeric))])
heatmap(cor(hp1))
heatmap(cor(hp1, is.numeric))
heatmap(cor(hp1[, unlist(lapply(hp1, is.numeric))]))
install.packages("metan")
install.packages("metan")
install.packages("metan")
setwd("C:/Users/Tia Phan/OneDrive/Máy tính/a/Dataset")
house.data=read.csv("kc_house_data.csv",header = TRUE)
table(house.data$floors)
house.data$floors=floor(house.data$floors)
table(house.data$floors)
floors=house.data%>%
dplyr::count(floors)%>%
dplyr::mutate(perc=n/sum(n)*100)
ggplot(data=floors,aes(x=floors, y=n))+geom_col(col="pink", fill="pink")+geom_text(aes(x=floors,y=n,label=paste0(n,"(",round(perc,1),"%)")))+theme_classic()
# Plot percent
library(ggplot2)
floors=house.data%>%
dplyr::count(floors)%>%
dplyr::mutate(perc=n/sum(n)*100)
ggplot(data=floors,aes(x=floors, y=n))+geom_col(col="pink", fill="pink")+geom_text(aes(x=floors,y=n,label=paste0(n,"(",round(perc,1),"%)")))+theme_classic()
floors=house.data%>%
dplyr::count(floors)%>%
dplyr::mutate(perc=n/sum(n)*100)
library(dplyr)
floors=house.data%>%
dplyr::count(floors)%>%
dplyr::mutate(perc=n/sum(n)*100)
ggplot(data=floors,aes(x=floors, y=n))+geom_col(col="pink", fill="pink")+geom_text(aes(x=floors,y=n,label=paste0(n,"(",round(perc,1),"%)")))+theme_classic()
data <- house.data %>% dplyr::select(-c(date))
corrplot(cor(data), method="number", type="lower")
# Mapping
library(leaflet)
pal = colorNumeric("YlOrRd", domain = coordinates_data$price)
int_map <- coordinates_data %>%
leaflet()%>%
addProviderTiles(providers$OpenStreetMap.Mapnik)%>%
addCircleMarkers(col = ~pal(price), opacity = 1.1, radius = 0.3) %>%
addLegend(pal = pal, values = ~price)
int_map
# Select price, lat, long
coordinates_data = dplyr::select(house.data, price, lat, long)
pal = colorNumeric("YlOrRd", domain = coordinates_data$price)
int_map <- coordinates_data %>%
leaflet()%>%
addProviderTiles(providers$OpenStreetMap.Mapnik)%>%
addCircleMarkers(col = ~pal(price), opacity = 1.1, radius = 0.3) %>%
addLegend(pal = pal, values = ~price)
int_map
set.seed(123)
split2=sample.split(house.data$price, SplitRatio = 0.8)
training_set2=subset(house.data, split2==TRUE)
test_set2=subset(house.data, split2==FALSE)
regressor1=lm(formula=price~., data=training_set2)
summary(regressor1)
# Prediction Linear
set.seed(123)
split2=sample.split(house.data$price, SplitRatio = 0.8)
library(caret)
split2=sample.split(house.data$price, SplitRatio = 0.8)
library(MASS) #Box-cox
library(car) #Vif
library(glmnet) #LASSO, Ridge
library(xgboost)
library(lubridate)
library(GGally)
library(cluster)
library(caTools)
library(pacman)
p_load('tidyverse','rpart','rpart.plot','Metrics','forecast','caret',
'ggplot2', 'FNN', 'fastDummies','dataPreparation','reshape2','corrplot')
split2=sample.split(house.data$price, SplitRatio = 0.8)
training_set2=subset(house.data, split2==TRUE)
test_set2=subset(house.data, split2==FALSE)
regressor1=lm(formula=price~., data=training_set2)
summary(regressor1)
# Delete insignificant variables. got regression 2
regressor2=lm(formula=price~bedrooms+bathrooms+sqft_living+waterfront+view+condition+grade+sqft_above+yr_built+yr_renovated+zipcode+lat+long+sqft_lot15, data=training_set2)
summary(regressor2)
pred=regressor2$fitted.values
tally_table=data.frame(actual=training_set2$price, predicted=pred)
mape=mean(abs(tally_table$actual-tally_table$predicted)/tally_table$actual)
accuracy=1-mape
cat("The accuracy on the train data is:",accuracy)
pred_test2=predict(newdata=test_set2,regressor2)
tally_table=data.frame(actual=test_set2$price, predicted=pred_test2)
mape=mean(abs(tally_table$actual-tally_table$predicted)/tally_table$actual)
accuracy=1-mape
cat(" and the accuracy on the test data is:",accuracy)
# Calculate the MSE
prednew=predict(regressor2,newdata = test_set2)
cor(prednew,test_set2$price)
residuals2=(prednew-test_set2$price)
mse2=mean(residuals2)
mse2
# Decision tree
library(tree)
set.seed(1234)
train=sample(1:nrow(house.data),nrow(house.data)/2)
tree.house=tree(price~grade+view+sqft_living+yr_built,house.data,subset=train)
summary(tree.house)
plot(tree.house)
text(tree.house)
tree.house
yhat=predict(tree.house,newdata = house.data[-train,])
house.test=house.data[-train,"price"]
mse=mean((yhat-house.test)^2)
mse
confMat <- table(house.data$price,yhat)
house.model=lm(price ~.,data= house.data)
summary(house.model)
cv.king <- cv.tree(tree.house)
plot(cv.king$size, cv.king$dev, type = "b")
text(tree.house)
tree.house
sqrt(mse)
library(readxl)
hap <- read_excel("C:/Users/Tia Phan/OneDrive/Máy tính/a/Dataset/happiness 2019-2020.xlsx")
setwd("C:/Users/Tia Phan/OneDrive/Máy tính/a/Dataset")
hap <- read_excel("C:/Users/Tia Phan/OneDrive/Máy tính/a/Dataset/happiness 2019-2020.xlsx")
hap <- read_csv("C:/Users/Tia Phan/OneDrive/Máy tính/a/Dataset/happiness 2019-2020.csv")
library(readr)
happ <- read_csv("C:/Users/Tia Phan/OneDrive/Máy tính/a/Dataset/Happiness Score/happiness 2019-2020.csv")
View(happ)
library(ggplot2)
library(dplyr)
library(arules)
library(arules)
library(tidyverse)
library(lubridate)
library(ggplot2)
library(dplyr)
library(tidyr)
library(sf)
library(ggcorrplot)
library(choroplethr)
library(ggmap)
install.packages("Sf")
